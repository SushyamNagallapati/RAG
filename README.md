
# ðŸ§  RAG (Retrieval-Augmented Generation) with LLaMA 3.2 and Ollama

This project is a Retrieval-Augmented Generation (RAG) assistant that answers questions using:

- ðŸ§  **LLaMA 3.2** via [Ollama](https://ollama.com)
- ðŸ”— **LangChain** for orchestration
- ðŸ§² **BGE-large embeddings** (BAAI/bge-large-en)
- ðŸ“¦ **ChromaDB** for vector storage
- ðŸ **Python**

It allows you to ask questions based on your **own PDF documents** using a **local LLM**.

---

## ðŸš€ Quickstart

### 1. ðŸ“¥ Clone the Repository

```bash
git clone https://github.com/SushyamNagallapati/RAG.git
cd RAG
```

### 2. ðŸ§  Install Ollama and Pull LLaMA 3.2

Install Ollama (macOS, Linux, Windows):  
ðŸ‘‰ https://ollama.com/download

Then pull the model:

```bash
ollama pull llama3
```

> ðŸŸ¡ Make sure Ollama is running in the background before you use the model.

---

### 3. ðŸ Create a Virtual Environment & Install Dependencies

```bash
python3 -m venv venv
source venv/bin/activate          # On Windows: .\venv\Scripts\activate
pip install -r requirements.txt
```

---

### 4. ðŸ“‚ Add Your PDFs

Put your PDF documents inside the `data/` folder.  
Example: `data/insurance_policy.pdf`

---

### 5. ðŸ§  Ingest Data into ChromaDB

Run the ingestion script:

```bash
python ingest.py
```

---

### 6. ðŸ’¬ Ask Questions (Use the RAG Assistant)

Make sure Ollama is running in the background.

```bash
# Terminal 1
ollama run llama3.2:latest

# Terminal 2 (after activating venv)
python rag.py
```

Then type your question when prompted!

---

## âœ… Example Questions You Can Ask

- Whatâ€™s covered under this health insurance policy?
- Is COVID-19 treatment included?
- Whatâ€™s the claim filing process?
- When does the coverage period start and end?
- Is dental coverage included?

---

## ðŸ—‚ï¸ File Structure

```
data/              # PDF documents
db/                # Chroma vector DB (auto-generated)
embeddings.py      # Embedding setup using HuggingFace
ingest.py          # PDF loader + ChromaDB population
llama_model.py     # Communicates with Ollama (LLaMA model)
retriever.py       # Finds relevant chunks
rag.py             # Main RAG orchestration loop
requirements.txt   # Python dependencies
.gitignore         # Ignored files/folders
```

---

## ðŸ› ï¸ Troubleshooting

- âŒ **KeyError: 'response'** â€” Make sure model name matches `ollama list` in `llama_model.py`
- âŒ **Too many files showing in Git** â€” Add `.gitignore` to exclude `venv/`, `db/`, `.DS_Store`, etc.
- âŒ **Missing embeddings** â€” Make sure `ingest.py` has been run and `db/` is created

---

## ðŸŒ± Future Improvements

- ðŸ”Œ Add FastAPI for tool calling
- ðŸ§® Enable schema-aware SQL generation
- ðŸŽ¨ Add a web UI using Streamlit or Gradio
- ðŸ“ Add citations with sources
- ðŸ“Š Optionally use Qdrant instead of Chroma

---

## ðŸ“ License

MIT â€” use it freely, modify it, and share it.

---

> ðŸ”§ Built with â¤ï¸ by [Sushyam Nagallapati](https://github.com/SushyamNagallapati)

> MEng System Design Engineering, University of Waterloo
